"""Regex-based LaTeX tokenizer.

Produces a flat stream of tokens from LaTeX source, designed for the
known subset generated by the Smart-LaTeX LLM pipeline.
"""

import re
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Iterator


class TokenType(Enum):
    COMMENT = auto()
    MATH_DISPLAY = auto()      # $$...$$ or \[...\]
    MATH_INLINE = auto()       # $...$
    ENV_BEGIN = auto()          # \begin{name}
    ENV_END = auto()            # \end{name}
    NEWLINE_CMD = auto()        # \\ or \\[1em]
    ITEM = auto()               # \item or \item[label]
    TOPRULE = auto()
    MIDRULE = auto()
    BOTTOMRULE = auto()
    CMIDRULE = auto()           # \cmidrule{2-3} or \cmidrule(lr){2-3}
    HLINE = auto()
    COMMAND = auto()            # \commandname or \command*
    BRACE_OPEN = auto()
    BRACE_CLOSE = auto()
    BRACKET_OPEN = auto()       # [
    BRACKET_CLOSE = auto()      # ]
    AMPERSAND = auto()
    TEXT = auto()
    WHITESPACE = auto()         # runs of whitespace (spaces, tabs)
    PAR_BREAK = auto()          # blank line (paragraph break)
    EOF = auto()


@dataclass
class Token:
    type: TokenType
    value: str
    pos: int
    extra: dict = field(default_factory=dict)

    def __repr__(self) -> str:
        extra_str = f", {self.extra}" if self.extra else ""
        return f"Token({self.type.name}, {self.value!r}{extra_str})"


# ---------------------------------------------------------------------------
# Master regex — order matters (first match wins)
# ---------------------------------------------------------------------------

_PATTERNS: list[tuple[str, TokenType, str | None]] = [
    # Escaped special characters: \%, \&, \#, \$, \_, \{, \}
    # Must come before COMMENT so \% is not treated as a comment
    (r"\\([%&#\$_\{\}])", TokenType.TEXT, None),

    # Comment: % to end of line
    (r"%[^\n]*", TokenType.COMMENT, None),

    # Paragraph break: two or more newlines (possibly with whitespace between)
    (r"\n[ \t]*\n[ \t\n]*", TokenType.PAR_BREAK, None),

    # Display math: $$...$$ (non-greedy)
    (r"\$\$(.+?)\$\$", TokenType.MATH_DISPLAY, None),

    # Display math: \[...\]
    (r"\\\[(.+?)\\\]", TokenType.MATH_DISPLAY, None),

    # Inline math: $...$ (not $$)
    (r"(?<!\$)\$(?!\$)(.+?)(?<!\$)\$(?!\$)", TokenType.MATH_INLINE, None),

    # \begin{envname} with optional * (e.g. \begin{equation*})
    (r"\\begin\{(\w+\*?)\}", TokenType.ENV_BEGIN, "name"),

    # \end{envname}
    (r"\\end\{(\w+\*?)\}", TokenType.ENV_END, "name"),

    # Newline command: \\ with optional [length]
    (r"\\\\(?:\[([^\]]*)\])?", TokenType.NEWLINE_CMD, "length"),

    # \item with optional [label]
    (r"\\item(?:\[([^\]]*)\])?", TokenType.ITEM, "label"),

    # Booktabs rules
    (r"\\toprule\b", TokenType.TOPRULE, None),
    (r"\\midrule\b", TokenType.MIDRULE, None),
    (r"\\bottomrule\b", TokenType.BOTTOMRULE, None),
    (r"\\cmidrule(?:\([^)]*\))?\{([^}]*)\}", TokenType.CMIDRULE, "range"),
    (r"\\hline\b", TokenType.HLINE, None),

    # Commands: \name or \name* (must come after specific commands above)
    (r"\\([a-zA-Z@]+\*?)", TokenType.COMMAND, "name"),

    # Structural characters
    (r"\{", TokenType.BRACE_OPEN, None),
    (r"\}", TokenType.BRACE_CLOSE, None),
    (r"\[", TokenType.BRACKET_OPEN, None),
    (r"\]", TokenType.BRACKET_CLOSE, None),
    (r"&", TokenType.AMPERSAND, None),

    # Whitespace (spaces/tabs within a line, or single newline)
    (r"[ \t]+|\n", TokenType.WHITESPACE, None),

    # Text: any character that doesn't start a special sequence
    # Matches runs of "normal" characters including CJK
    (r"[^\\\$%\{\}\[\]&\n \t~]+", TokenType.TEXT, None),

    # Tilde (non-breaking space) → treat as text space
    (r"~", TokenType.TEXT, None),
]

# Compile all patterns into a single regex with named groups
_TOKEN_RE = re.compile(
    "|".join(
        f"(?P<T{i}>{pat})" for i, (pat, _, _) in enumerate(_PATTERNS)
    ),
    re.DOTALL,
)


def tokenize(source: str) -> Iterator[Token]:
    """Yield tokens from *source* LaTeX string."""
    for m in _TOKEN_RE.finditer(source):
        # Find which alternative matched
        for i, (_, tok_type, extra_key) in enumerate(_PATTERNS):
            group_name = f"T{i}"
            val = m.group(group_name)
            if val is not None:
                extra = {}
                if extra_key is not None:
                    # The first capturing group inside the pattern
                    # We need to find it by index within the named group
                    # Use the match groups
                    inner_groups = [
                        g for g in m.groups()
                        if g is not None and g != val
                    ]
                    # Better approach: use the pattern's group directly
                    sub_match = re.match(
                        _PATTERNS[i][0], val, re.DOTALL
                    )
                    if sub_match and sub_match.lastindex:
                        extra[extra_key] = sub_match.group(1) or ""
                    elif extra_key == "label":
                        # \item without [] — no label
                        extra[extra_key] = None

                # For escaped special chars (\%, \&, etc.), use just the char
                if i == 0 and tok_type == TokenType.TEXT:
                    # Pattern 0 is the escaped specials pattern
                    sub = re.match(_PATTERNS[0][0], val)
                    if sub:
                        val = sub.group(1)  # just the character

                # For MATH tokens, store the math content in extra
                if tok_type == TokenType.MATH_DISPLAY:
                    sub = re.match(_PATTERNS[i][0], val, re.DOTALL)
                    if sub:
                        extra["content"] = sub.group(1)
                elif tok_type == TokenType.MATH_INLINE:
                    sub = re.match(_PATTERNS[i][0], val, re.DOTALL)
                    if sub:
                        extra["content"] = sub.group(1)

                yield Token(
                    type=tok_type,
                    value=val,
                    pos=m.start(),
                    extra=extra,
                )
                break

    yield Token(type=TokenType.EOF, value="", pos=len(source))
